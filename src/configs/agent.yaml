# Example Flexo Configuration
# This configuration demonstrates key features and common settings

# Basic agent configuration
name: flexo
history_limit: 3 # You can set to 1 to disable multi-turn
max_streaming_iterations: 4 # Number of times the agent is allowed to return to streaming (e.g. after tools execution)

# Response timeouts
timeouts:
  model_response_timeout: 60

# CORS allowed origins (optional)
allowed_origins:
  - http://localhost:8080 # example for local Open WebUI

# System prompt defining agent behavior and capabilities
system_prompt: |
  I am a helpful AI assistant focused on clear, accurate, and direct communication. 
  I solve problems systematically and explain my reasoning when needed. 
  I maintain a professional yet approachable tone.
  
  CORE BEHAVIORS:
  - Communicate clearly and concisely
  - Break down complex problems step-by-step
  - Ask clarifying questions when truly needed
  - Acknowledge limitations and uncertainties honestly
  - Validate assumptions before proceeding
  
  TOOL USAGE:
  - Call tool(s) immediately as needed 
  - Do not reference tools or their formatting in your response
  - Use tools only when they genuinely enhance the response
  - Handle errors gracefully with clear explanations
  - Show key results and interpret them in context
  - Suggest alternatives if the primary approach fails
  
  I adapt my style to user needs while staying focused on providing valuable, 
  actionable responses.

# Core behavior settings
detection_mode: vendor               # Options: manual, vendor
use_vendor_chat_completions: true    # Options: true, false

# Model Configuration
models_config:
  main_chat_model:
    # WatsonX Configuration
#    vendor: watsonx-llama # Options: watsonx-llama/granite/mistral, openai, anthropic, mistral-ai
#    model_id: meta-llama/llama-3-405b-instruct # Other options: mistralai/mistral-large, meta-llama/llama-3-405b-instruct, ibm/granite-3-8b-instruct
#    decoding_method: greedy
#    max_new_tokens: 4000

    # Alternative OpenAI Configuration (uncomment to use)
     vendor: openai
     model_id: gpt-4o-mini
     max_tokens: 4000
     temperature: 0.7

     # Alternative Anthropic Configuration (uncomment to use)
#     vendor: anthropic
#     model_id: claude-3-5-sonnet-latest
#     max_tokens: 4000
#     temperature: 0.7

     # Alternative xAI Configuration (uncomment to use)
#      vendor: xai
#      model_id: grok-2-latest
#      temperature: 0.7
#      max_tokens: 2000
#
     # Alternative Mistral AI Configuration (uncomment to use)
#      vendor: mistral-ai
#      model_id: mistral-large-latest
#      temperature: 0.7
#      max_tokens: 4096

     # Alternative vLLM Configuration (uncomment to use)
#     vendor: openai-compat-llama
#     base_url: http://0.0.0.0:8000/v1
#     model_id: meta-llama/Llama-3.2-3B-Instruct
#     max_tokens: 2000
#     temperature: 0.7

     # Alternative ollama Configuration (uncomment to use)
#     vendor: openai-compat-granite
#     base_url: http://0.0.0.0:11434/v1
#     model_id: granite31
#     api_key: ollama
#     max_tokens: 2000
#     temperature: 0.7

# MCP configuration
mcp_config:
  # Example SSE
  transport: sse
  sse_url: "http://localhost:8001/sse"
  sampling_enabled: false

  # Example stdio
#  transport: stdio
#  command: python
#  args: ["weather/weather.py"]
#  env: null
#  sampling_enabled: false

# Tool Configurations
tools_config:
  # Weather API Integration
  - name: "weather"
    endpoint_url: "https://api.openweathermap.org/data/2.5/weather"
    api_key_env: "OWM_API_KEY"

  # Wikipedia Summary Tool
  - name: "wikipedia"
    endpoint_url: "https://{lang}.wikipedia.org/api/rest_v1/page/summary/{encoded_query}"

  # DuckDuckGo Search Tool
  - name: "duckduckgo_search"

  # RAG Tool with Elasticsearch (optional, not enabled by default)
#  - name: "medicare_search"
#    connector_config:
#      connector_type: elasticsearch
#      index_name: my_index
#      api_key_env: ES_API_KEY
#      endpoint_env: ES_ENDPOINT
#      top_k: 5
#      timeout: 30
#      max_retries: 3
#      query_body:
#        _source: false
#        fields: ["text"]
#        query:
#          bool:
#            must:
#              - match:
#                  text:
#                    query: "$USER_INPUT"
#                    boost: 3.5
#        knn:
#          field: vector
#          query_vector_builder:
#            text_embedding:
#              model_id: thenlper__gte-base
#              model_text: "$USER_INPUT"
#          k: 100
#          num_candidates: 150
#        rank:
#          rrf:
#            rank_window_size: 40
